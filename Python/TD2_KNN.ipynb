{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b3ce168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0c5ca",
   "metadata": {},
   "source": [
    "# Introduction : Présentation de l'exercice\n",
    "Dans cet exercice, vous allez explorer les données IRIS, un dataset classique utilisé en apprentissage automatique.  Vous apprendrez à charger, visualiser, normaliser les données et à implémenter votre propre algorithme des K plus proches voisins (KNN) sous forme de classe. Vous serez également guidés pour évaluer la performance de votre modèle et interpréter les résultats. \n",
    "Les objectifs principaux sont :\n",
    "1. Comprendre les étapes de prétraitement des données.\n",
    "2. Apprendre à visualiser les données en utilisant Matplotlib.\n",
    "3. Implémenter un modèle KNN simple sans utiliser de bibliothèques externes.\n",
    "4. Évaluer la performance de votre modèle en utilisant des métriques comme la précision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d9365",
   "metadata": {},
   "source": [
    "# Étape 1 : Chargement du dataset IRIS\n",
    "1. Importez le module `load_iris` depuis `sklearn.datasets`.\n",
    "2. Chargez le dataset IRIS en appelant la fonction `load_iris()` et stockez-le dans une variable (par exemple, `iris`).\n",
    "3. Extraire les données des caractéristiques (features) sous forme de DataFrame pandas et les stocker dans une variable `X`. Les colonnes du DataFrame doivent correspondre aux noms des caractéristiques disponibles dans `iris['feature_names']`.\n",
    "4. Extraire les cibles (targets) sous forme de Série pandas et les stocker dans une variable `y`. Assurez-vous que la Série a un nom explicite, par exemple, \"target\".\n",
    "5. Affichez un aperçu des données de `X` et `y` en utilisant les méthodes `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb24cab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00acd42e",
   "metadata": {},
   "source": [
    "# Étape 2 : Préparation des données\n",
    "Instructions pour les étudiants :\n",
    "1. Normalisez les données des caractéristiques pour que leurs valeurs soient comprises entre 0 et 1.\n",
    "2. Suivez les étapes suivantes pour normaliser :\n",
    "    - Calculez la valeur minimale de chaque caractéristique en utilisant `X.min()`.\n",
    "    - Calculez la valeur maximale de chaque caractéristique en utilisant `X.max()`.\n",
    "    - Appliquez la formule de normalisation pour chaque caractéristique :\n",
    "    normalized_value = (value - min) / (max - min)\n",
    "    - Remplacez les valeurs de `X` par leurs valeurs normalisées.\n",
    "3. Une fois la normalisation terminée, affichez un aperçu des données normalisées de `X` en utilisant `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccb61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9da6563e",
   "metadata": {},
   "source": [
    "Utiliser de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27de43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80b73eb9",
   "metadata": {},
   "source": [
    "# Étape 3 : Visualisation des données avec Matplotlib\n",
    "Vous allez utiliser Matplotlib pour visualiser la distribution des données. Suivez les étapes ci-dessous :\n",
    "1. Créez une nouvelle fonction appelée `plot_scatter` qui accepte les paramètres suivants :\n",
    "    - X : les données d'entrée (features).\n",
    "    - y : les cibles (targets).\n",
    "    - feature_x : l'indice de la première caractéristique à tracer.\n",
    "    - feature_y : l'indice de la seconde caractéristique à tracer.\n",
    "    - feature_names : la liste des noms des caractéristiques.\n",
    "    - target_names : la liste des noms des classes cibles.\n",
    "    - colors : une liste de couleurs correspondant à chaque classe cible.\n",
    "    - title : le titre du graphique.\n",
    "2. Utilisez plt.figure pour définir la taille du graphique.\n",
    "3. Parcourez chaque classe cible (0, 1, 2) et :\n",
    "    - Sélectionnez les lignes de X où y correspond à la classe actuelle.\n",
    "    - Tracez un nuage de points (scatter) pour les deux caractéristiques sélectionnées, en utilisant une couleur spécifique pour chaque classe.\n",
    "4. Ajoutez des étiquettes pour les axes x et y en utilisant plt.xlabel et plt.ylabel.\n",
    "5. Ajoutez un titre avec plt.title et une légende avec plt.legend.\n",
    "6. Affichez le graphique en utilisant plt.show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db1bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c950823",
   "metadata": {},
   "source": [
    "Créez la liste des couleurs, des noms de caractéristiques et des noms des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568820a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "573ff503",
   "metadata": {},
   "source": [
    "Visualisation de \"sepal length (cm)\" vs \"sepal width (cm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849c984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86586c35",
   "metadata": {},
   "source": [
    "Visualisation de \"petal length (cm)\" vs \"petal width (cm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b095e8ca",
   "metadata": {},
   "source": [
    "Visualisation croisée de \"sepal length (cm)\" vs \"petal length (cm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0aa846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc1d3e51",
   "metadata": {},
   "source": [
    "Visualisation croisée de \"sepal width (cm)\" vs \"petal width (cm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a828c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a3e33c6",
   "metadata": {},
   "source": [
    "# Étape 4 : Visualisation des histogrammes par caractéristique et par classe\n",
    "Vous allez implémenter une fonction appelée `plot_histograms_by_target` qui affiche un histogramme de la distribution des valeurs pour chaque caractéristique, groupées par classe cible. Suivez les étapes ci-dessous :\n",
    "1. Créez une fonction nommée plot_histograms_by_target acceptant les arguments suivants :\n",
    "    - X : un DataFrame contenant les caractéristiques.\n",
    "    - y : une Série ou un tableau contenant les cibles.\n",
    "    - feature_names : une liste des noms des caractéristiques.\n",
    "    - target_names : une liste des noms des classes cibles.\n",
    "    - colors : une liste des couleurs associées à chaque classe.\n",
    "2. Dans la fonction, pour chaque caractéristique (parcourue à l'aide de feature_names) :\n",
    "    - Créez une figure avec plt.figure.\n",
    "    - Parcourez chaque classe cible et :\n",
    "      - Sélectionnez les lignes de X correspondant à cette classe.\n",
    "      - Tracez un histogramme pour la caractéristique courante avec plt.hist, en utilisant une couleur spécifique.\n",
    "    - Ajoutez des étiquettes pour les axes, un titre pour l'histogramme, et une légende pour indiquer les classes.\n",
    "3. Appelez cette fonction avec les données fournies pour générer les histogrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7638903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8adea2e",
   "metadata": {},
   "source": [
    "Apeller de la fonction pour tracer les histogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cb92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6571be78",
   "metadata": {},
   "source": [
    "# Étape 5 : Implémentation de l'algorithme des KNN sous forme de classe\n",
    "Implémentez une classe `KNNClassifier` qui inclut les méthodes suivantes :\n",
    "- __init__(self, k) : pour initialiser le nombre de voisins k.\n",
    "- fit(self, X_train, y_train) : pour stocker les données d'entraînement.\n",
    "- predict(self, X_test) : pour prédire les classes des données test en suivant ces étapes :\n",
    "   1. Pour chaque point de test, calculer la distance à chaque point d'entraînement.\n",
    "      Cette distance doit être calculée à l'aide d'une méthode privée comme suit :\n",
    "      distance = sqrt((x1 - x2)^2 + (y1 - y2)^2 + ...).\n",
    "   2. Identifier les k voisins les plus proches en triant les distances.\n",
    "   3. Déterminer la classe majoritaire parmi les k voisins et l'assigner au point de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae509c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b08dc34",
   "metadata": {},
   "source": [
    "# Étape 6 : Utilisation du modèle\n",
    "1. Initialiser du classificateur avec k=3\n",
    "2. En utilisant la fonction `train_test_split` qui se trouve dans la bibliothèque `sklearn.model_selection`, séparer le dataset X & y en un dataset d'entrainement `X_train` & `y_train` et un dataset de test `X_test` & `y_test`.\n",
    "3. Entrainer le modèle avec les données d'entrainement\n",
    "4. Effectuer les prédictions sur les données de test.\n",
    "5. Afficher les prédictions réalisées avec la vérité terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959290a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd52e6ef",
   "metadata": {},
   "source": [
    "# Étape 7 : Evaluation du modèle\n",
    "Implémentez une fonction `calculate_accuracy` qui calcule la précision d'un modèle.\n",
    " - La fonction prend deux arguments :\n",
    "   1. y_true : les vraies classes sous forme de liste ou tableau.\n",
    "   2. y_pred : les classes prédites sous forme de liste ou tableau.\n",
    " - La fonction retourne un nombre entre 0 et 1 représentant la proportion des prédictions correctes.\n",
    " - Étapes à suivre :\n",
    "   1. Initialiser un compteur à 0 pour les prédictions correctes.\n",
    "   2. Parcourir les deux listes en parallèle (y_true et y_pred).\n",
    "   3. Incrémenter le compteur à chaque fois que les deux valeurs correspondantes sont égales.\n",
    "   4. Diviser le compteur par la longueur de y_true pour obtenir la précision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dccb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fb5cae1",
   "metadata": {},
   "source": [
    "Afficher la précision du modèle sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63441094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04759e36",
   "metadata": {},
   "source": [
    "# Pour les plus rapides (à faire à la maison)\n",
    "1. Modifier le notebook (après l'avoir dupliqué!) pour explorer le dataset wine (load_wine dans sklearn.dataset).\n",
    "2. Créer une classe `KNNRegressor`, en vous appuyant sur le code que vous avez précédemment réalisé. Un régresseur permet de trouver un valeur numérique (entier ou réel) à la place d'une classe. Instructions\n",
    "    - le mécanisme de recherche des k plus proche voisin est le même\n",
    "    - à la place de faire un vote à la majorité, on calcule et retourne la moyenne des k points les plus proches de la donnée à prédire\n",
    "    - tester sur le dataset diabetes (toujours dans sklearn.dataset)\n",
    "    - l'évaluation se fait en calculant la moyenne des erreurs absolues entre la prédiction et la vérité terrain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
