{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Fall 2022\n",
    "# Homework 3 - extra credit\n",
    "# Due: Sunday September 18, 2022 at 6:00 PM\n",
    "\n",
    "This assignment is worth 10 points applied to your homework 2 grade. If you receive a 90+ on homework 2, you can still get the 10 extra points. In this assignment, I'm providing the solution to homework 2 and asking you to reflect on the Q-learning portion of the solution. The code is setup to run 3 trials, 10 steps each, and is hard-coded to start at the same location on each trial. It won't find the landing pad in only 3 trials, you will need to change that number if you want to see the code produce a valid solution. I added a few print statements in the Q-learning algorithm to help illustrate the code. Feel free to add as many additional print statements that you need to help you understand how its working.  \n",
    "\n",
    "<br>\n",
    "The questions are listed in Part E of this notebook.\n",
    "<br> \n",
    "\n",
    "### Your name:\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## [100 pts] Problem 1:  Reinforcement learning\n",
    "\n",
    "Consider a **cube** state space defined by $0 \\le x, y, z \\le L$. Suppose you are piloting/programming a drone to learn how to land on a platform at the center of the $z=0$ surface (the bottom). Some assumptions:\n",
    "* In this discrete world, if I say the drone is at $(x,y,z)$ I mean that it is in the box centered at $(x,y,z)$. And there are boxes (states) centered at $(x,y,z)$ for all $0 \\le x,y,z \\le L$. Each state is a 1 unit cube. So when $L=2$ (for example), there are cubes centered at each $x=0,1,2$, $y=0,1,2$ and so on, for a total state space size of $3^3 = 27$ states.\n",
    "* All of the states with $z=0$ are terminal states.\n",
    "* The state at the center of the bottom of the cubic state space is the landing pad. So, for example, when $L=4$, the landing pad is at $(x,y,z) = (2,2,0)$.\n",
    "* All terminal states ***except*** the landing pad have a reward of -1. The landing pad has a reward of +1.\n",
    "* All non-terminal states have a reward of -0.01.\n",
    "* The drone takes up exactly 1 cubic unit, and begins in a random non-terminal state.\n",
    "* The available actions in non-terminal states include moving exactly 1 unit Up (+z), Down (-z), North (+y), South (-y), East (+x) or West (-x). In a terminal state, the training episode should end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Write a class `MDPLanding` to represent the Markov decision process for this drone. Include methods for:\n",
    "1. `actions(state)`, which should return a list of all actions available from the given state\n",
    "2. `reward(state)`, which should return the reward for the given state\n",
    "3. `result(state, action)`, which should return the resulting state of doing the given action in the given state\n",
    "\n",
    "and attributes for:\n",
    "1. `states`, which is just a list of all the states in the state space, where each state is represented as an $(x,y,z)$ tuple\n",
    "2. `terminal_states`, a dictionary where keys are the terminal state tuples and the values are the rewards associated with those terminal states\n",
    "3. `default_reward`, which is a scalar for the reward associated with non-terminal states\n",
    "4. `all_actions`, a list of all possible actions (Up, Down, North, South, East, West)\n",
    "5. `discount`, the discount factor (use $\\gamma = 0.999$ for this entire problem)\n",
    "\n",
    "How you feed arguments/information into the class constructor is up to you.\n",
    "\n",
    "Note that actions are *deterministic* here.  The drone does not need to learn transition probabilities for outcomes of particular actions. What the drone does need to learn, however, is where the heck that landing pad is, and how to get there from any initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "\n",
    "class MDPLanding:\n",
    "    def __init__(self, size, default_reward, actions, discount):\n",
    "        self.size = size\n",
    "        self.lz = (int(size/2), int(size/2), 0)\n",
    "        self.states = [(x,y,z) for x in range(size) for y in range(size) for z in range(size)]\n",
    "        self.terminal_states = {(x,y,0) : -1 for x in range(size) for y in range(size)}\n",
    "        self.terminal_states[self.lz] = +1\n",
    "        self.default_reward = default_reward\n",
    "        self.all_actions = actions\n",
    "        self.discount = discount\n",
    "\n",
    "    def actions(self, state):\n",
    "        '''all are available, unless you are in a terminal state\n",
    "        (the drone might bump into wall in some cases though)'''\n",
    "        if state in self.terminal_states:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_actions\n",
    "        \n",
    "    def reward(self, state):\n",
    "        return self.terminal_states[state] if state in self.terminal_states.keys() else self.default_reward\n",
    "                \n",
    "    def result(self, state, action):\n",
    "        '''result of doing `action` in `state`, deterministic.\n",
    "        `action` is one of N, S, E, W, U, D (as string)'''\n",
    "        \n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "        \n",
    "        if action=='N':\n",
    "            new_state = (state[0], state[1]+1, state[2])\n",
    "        elif action=='S':\n",
    "            new_state = (state[0], state[1]-1, state[2])\n",
    "        elif action=='E':\n",
    "            new_state = (state[0]+1, state[1], state[2])\n",
    "        elif action=='W':\n",
    "            new_state = (state[0]-1, state[1], state[2])\n",
    "        elif action=='U':\n",
    "            new_state = (state[0], state[1], state[2]+1)\n",
    "        elif action=='D':\n",
    "            new_state = (state[0], state[1], state[2]-1)\n",
    "        elif action is None:\n",
    "            return state\n",
    "        return new_state if new_state in self.states else state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part B\n",
    "Write a function to implement **policy iteration** for this drone landing MDP. Create an MDP environment to represent the $L=4$ case (so 125 total states).\n",
    "\n",
    "Use your function to find an optimal policy for your new MDP environment. Check (by printing to screen) that the policy for the following states are what you expect, and comment on the results:\n",
    "1. $(2,2,1)$\n",
    "1. $(0,2,1)$\n",
    "1. $(2,0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy(2,2,1) = D\n",
      "Policy(0,2,1) = E\n",
      "Policy(2,0,1) = N\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(mdp):\n",
    "    # initilize utility for all states\n",
    "    utility = {s : 0 for s in mdp.states}\n",
    "    # initialize a policy for each state, being a random action\n",
    "    policy = {s: np.random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        # policy evaluation step\n",
    "        utility = policy_evaluation(policy, utility, mdp)\n",
    "        # policy improvement step\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            best = (-999, None)\n",
    "            for a in mdp.actions(s):\n",
    "                new_util = utility[mdp.result(s,a)]\n",
    "                if new_util > best[0]:\n",
    "                    best = (new_util, a)\n",
    "            if best[1] != policy[s]:\n",
    "                policy[s] = best[1]\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return policy\n",
    "\n",
    "def policy_evaluation(policy, utility, mdp, n_iter=50):\n",
    "    '''Do a handful (n_iter) of value iterations to update the\n",
    "    utility of each state, under the given policy'''\n",
    "    for i in range(n_iter):\n",
    "        for s in mdp.states:\n",
    "            if s in mdp.terminal_states:\n",
    "                utility[s] = mdp.reward(s)\n",
    "            else:\n",
    "                utility[s] = mdp.reward(s) + mdp.discount * utility[mdp.result(s, policy[s])]\n",
    "    return utility\n",
    "\n",
    "size = 5\n",
    "default_reward = -0.01\n",
    "discount = 0.999\n",
    "actions = ['N','S','E','W','U','D']\n",
    "mdp = MDPLanding(size, default_reward, actions, discount)\n",
    "\n",
    "policy = policy_iteration(mdp)\n",
    "print('Policy(2,2,1) = '+policy[(2,2,1)])\n",
    "print('Policy(0,2,1) = '+policy[(0,2,1)])\n",
    "print('Policy(2,0,1) = '+policy[(2,0,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Makes sense, because (2,2,1) is right above the landing pad in the 5x5x5 environment, so the drone should just go straight down. Similarly, (0,2,1) is West of the landing pad (lower x-value), so the drone needs to head toward +x, or East, and (2,0,1) is South of the landing pad (lower y-value), so the drone needs to head toward +y, or North."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Code up a **Q-learning** agent/algorithm to learn how to land the drone. You can do this however you like, as long as you use the MDP class structure defined above.  \n",
    "\n",
    "Your code should include some kind of a wrapper to run many trials to train the agent and learn the Q values (see Section 22.3 in the textbook - page 803 might be of particular interest).  You also do not need to have a separate function for the actual \"agent\"; your code can just be a \"for\" loop within which you are refining your estimate of the Q values.\n",
    "\n",
    "From each training trial, save the cumulative discounted reward (utility) over the course of that episode. That is, add up all of $\\gamma^t R(s_t)$ where the drone is in state $s_t$ during time step $t$, for the entire sequence. I refer to this as \"cumulative reward\" because we usually refer to \"utility\" as the utility *under an optimal policy*.\n",
    "\n",
    "Some guidelines:\n",
    "* The drone should initialize in a random non-terminal state for each new training episode.\n",
    "* The training episodes should be limited to 50 time steps, even if the drone has not yet landed. If the drone lands (in a terminal state), the training episode is over.\n",
    "* You may use whatever learning rate $\\alpha$ you decide is appropriate, and gives good results.\n",
    "* There are many forms of Q-learning. You can use whatever you would like, subject to the reliability targets in Part D below.\n",
    "* Your code should return:\n",
    "  * The learned Q values associated with each state-action pair.\n",
    "  * The cumulative reward for each training trial. \n",
    "  * Anything else that might be useful in the ensuing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "\n",
    "def run_many_trials(mdp, n_trials, n_steps=None):\n",
    "\n",
    "    Q = defaultdict(float)   # state-action matrix\n",
    "    Nsa = defaultdict(float) # state-action visit counter (exploration)\n",
    "    rewards = []\n",
    "    epsilon = 0.1 # for epsilon-greedy action\n",
    "    alpha = lambda n: 1./(1+n) # learning rate related to number of visits to this state\n",
    "    policy = {state : np.random.choice(mdp.all_actions) for state in mdp.states} # random initial policy\n",
    "        \n",
    "    for k in range(n_trials):\n",
    "        \n",
    "        #s = mdp.states[np.random.randint(len(mdp.states))] # random initial location\n",
    "        s = mdp.states[148] #seems like a nice starting place in the middle of the state space\n",
    "        print('mdp.states[148]:')\n",
    "        print(s)\n",
    "        print('trial: ', k)\n",
    "        while s in mdp.terminal_states:\n",
    "            s = mdp.states[np.random.randint(len(mdp.states))] # re-sample if terminal\n",
    "        r = mdp.reward(s)\n",
    "        cumulative_reward = 0\n",
    "                \n",
    "        for t in range(n_steps):\n",
    "\n",
    "            if s in mdp.terminal_states:\n",
    "                Q[s, None] = r\n",
    "                # update the policy\n",
    "                policy[s] = None\n",
    "                print('Terminal found:')\n",
    "                print(s)\n",
    "                cumulative_reward += (mdp.discount**t)*r\n",
    "                break\n",
    "            else:\n",
    "                # pick a new action and state\n",
    "                print('policy: ' + policy[s])\n",
    "                a = return_epsilon_greedy_action(policy, s, mdp, epsilon)\n",
    "                print(\"action: \" + a)\n",
    "                print(a)\n",
    "                s1 = mdp.result(s, a)\n",
    "                print(\"s1:\")\n",
    "                print(s1)\n",
    "                r1 = mdp.reward(s1)\n",
    "                print(\"r1:\")\n",
    "                print(r1)\n",
    "                # update the state-action \"matrix\"\n",
    "                Nsa[s, a] += 1\n",
    "                print(\"Q[s, a] before for s and a:\", Q[s,a], s, a)\n",
    "                Q[s, a] += alpha(Nsa[s, a]) * (r + mdp.discount * max(Q[s1, a1] for a1 in mdp.actions(s1)) - Q[s, a])\n",
    "                print(\"Q[s, a] for s and a:\", Q[s,a], s, a)\n",
    "                for a1 in mdp.actions(s1):\n",
    "                    print(\"Q for a1, s1\", Q[s1, a1], s1, a1)\n",
    "                #                Q[s, a] += 0.001 * (r + mdp.discount * max(Q[s1, a1] for a1 in mdp.actions(s1)) - Q[s, a])\n",
    "                # update the policy\n",
    "                best = (-999, None)\n",
    "                for a1 in mdp.actions(s):\n",
    "                    new = Q[s, a1]\n",
    "                    if new > best[0]:\n",
    "                        best = (new, a1)\n",
    "                policy[s] = best[1]\n",
    "                cumulative_reward += (mdp.discount**t)*r\n",
    "                s = s1\n",
    "                r = r1\n",
    "\n",
    "        rewards.append(cumulative_reward)\n",
    "\n",
    "    return Q, Nsa, rewards\n",
    "\n",
    "def return_epsilon_greedy_action(policy, state, mdp, epsilon=0.1):\n",
    "    '''Return a random action or the one with highest utility (so far)'''\n",
    "    if np.random.uniform(0, 1) <= epsilon:\n",
    "        action = np.random.choice(mdp.actions(state))\n",
    "    else:\n",
    "        action = policy[state]\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Initialize the $L=10$ environment (so that the landing pad is at $(5,5,0)$). Run some number of training trials to train the drone.\n",
    "\n",
    "**How do I know if my drone is learned enough?**  If you take the mean cumulative reward across the last 5000 training trials, it should be around 0.80. This means at least about 10,000 (but probably more) training episodes will be necessary. It will take a few seconds on your computer, so start small to test your codes.\n",
    "\n",
    "**Then:** Compute block means of cumulative reward from all of your training trials. Use blocks of 500 training trials. This means you need to create some kind of array-like structure such that its first element is the mean of the first 500 trials' cumulative rewards; its second element is the mean of the 501-1000th trials' cumulative rewards; and so on. Make a plot of the block mean rewards as the training progresses. It should increase from about -0.5 initially to somewhere around +0.8.\n",
    "\n",
    "**And:** Print to the screen the mean of the last 5000 trials' cumulative rewards, to verify that it is indeed about 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.states[148]:\n",
      "(1, 2, 5)\n",
      "trial:  0\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(1, 2, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 5) U\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 5) U\n",
      "Q for a1, s1 0.0 (1, 2, 6) N\n",
      "Q for a1, s1 0.0 (1, 2, 6) S\n",
      "Q for a1, s1 0.0 (1, 2, 6) E\n",
      "Q for a1, s1 0.0 (1, 2, 6) W\n",
      "Q for a1, s1 0.0 (1, 2, 6) U\n",
      "Q for a1, s1 0.0 (1, 2, 6) D\n",
      "policy: W\n",
      "action: S\n",
      "S\n",
      "s1:\n",
      "(1, 1, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 6) S\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 6) S\n",
      "Q for a1, s1 0.0 (1, 1, 6) N\n",
      "Q for a1, s1 0.0 (1, 1, 6) S\n",
      "Q for a1, s1 0.0 (1, 1, 6) E\n",
      "Q for a1, s1 0.0 (1, 1, 6) W\n",
      "Q for a1, s1 0.0 (1, 1, 6) U\n",
      "Q for a1, s1 0.0 (1, 1, 6) D\n",
      "policy: W\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(0, 1, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 1, 6) W\n",
      "Q[s, a] for s and a: -0.005 (1, 1, 6) W\n",
      "Q for a1, s1 0.0 (0, 1, 6) N\n",
      "Q for a1, s1 0.0 (0, 1, 6) S\n",
      "Q for a1, s1 0.0 (0, 1, 6) E\n",
      "Q for a1, s1 0.0 (0, 1, 6) W\n",
      "Q for a1, s1 0.0 (0, 1, 6) U\n",
      "Q for a1, s1 0.0 (0, 1, 6) D\n",
      "policy: W\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(0, 2, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 1, 6) N\n",
      "Q[s, a] for s and a: -0.005 (0, 1, 6) N\n",
      "Q for a1, s1 0.0 (0, 2, 6) N\n",
      "Q for a1, s1 0.0 (0, 2, 6) S\n",
      "Q for a1, s1 0.0 (0, 2, 6) E\n",
      "Q for a1, s1 0.0 (0, 2, 6) W\n",
      "Q for a1, s1 0.0 (0, 2, 6) U\n",
      "Q for a1, s1 0.0 (0, 2, 6) D\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(0, 2, 7)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 2, 6) U\n",
      "Q[s, a] for s and a: -0.005 (0, 2, 6) U\n",
      "Q for a1, s1 0.0 (0, 2, 7) N\n",
      "Q for a1, s1 0.0 (0, 2, 7) S\n",
      "Q for a1, s1 0.0 (0, 2, 7) E\n",
      "Q for a1, s1 0.0 (0, 2, 7) W\n",
      "Q for a1, s1 0.0 (0, 2, 7) U\n",
      "Q for a1, s1 0.0 (0, 2, 7) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(0, 2, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 2, 7) D\n",
      "Q[s, a] for s and a: -0.005 (0, 2, 7) D\n",
      "Q for a1, s1 0.0 (0, 2, 6) N\n",
      "Q for a1, s1 0.0 (0, 2, 6) S\n",
      "Q for a1, s1 0.0 (0, 2, 6) E\n",
      "Q for a1, s1 0.0 (0, 2, 6) W\n",
      "Q for a1, s1 -0.005 (0, 2, 6) U\n",
      "Q for a1, s1 0.0 (0, 2, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(0, 3, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 2, 6) N\n",
      "Q[s, a] for s and a: -0.005 (0, 2, 6) N\n",
      "Q for a1, s1 0.0 (0, 3, 6) N\n",
      "Q for a1, s1 0.0 (0, 3, 6) S\n",
      "Q for a1, s1 0.0 (0, 3, 6) E\n",
      "Q for a1, s1 0.0 (0, 3, 6) W\n",
      "Q for a1, s1 0.0 (0, 3, 6) U\n",
      "Q for a1, s1 0.0 (0, 3, 6) D\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(0, 3, 7)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 3, 6) U\n",
      "Q[s, a] for s and a: -0.005 (0, 3, 6) U\n",
      "Q for a1, s1 0.0 (0, 3, 7) N\n",
      "Q for a1, s1 0.0 (0, 3, 7) S\n",
      "Q for a1, s1 0.0 (0, 3, 7) E\n",
      "Q for a1, s1 0.0 (0, 3, 7) W\n",
      "Q for a1, s1 0.0 (0, 3, 7) U\n",
      "Q for a1, s1 0.0 (0, 3, 7) D\n",
      "policy: W\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(0, 3, 7)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 3, 7) W\n",
      "Q[s, a] for s and a: -0.005 (0, 3, 7) W\n",
      "Q for a1, s1 0.0 (0, 3, 7) N\n",
      "Q for a1, s1 0.0 (0, 3, 7) S\n",
      "Q for a1, s1 0.0 (0, 3, 7) E\n",
      "Q for a1, s1 -0.005 (0, 3, 7) W\n",
      "Q for a1, s1 0.0 (0, 3, 7) U\n",
      "Q for a1, s1 0.0 (0, 3, 7) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(0, 4, 7)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 3, 7) N\n",
      "Q[s, a] for s and a: -0.005 (0, 3, 7) N\n",
      "Q for a1, s1 0.0 (0, 4, 7) N\n",
      "Q for a1, s1 0.0 (0, 4, 7) S\n",
      "Q for a1, s1 0.0 (0, 4, 7) E\n",
      "Q for a1, s1 0.0 (0, 4, 7) W\n",
      "Q for a1, s1 0.0 (0, 4, 7) U\n",
      "Q for a1, s1 0.0 (0, 4, 7) D\n",
      "mdp.states[148]:\n",
      "(1, 2, 5)\n",
      "trial:  1\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 3, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 5) N\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 5) N\n",
      "Q for a1, s1 0.0 (1, 3, 5) N\n",
      "Q for a1, s1 0.0 (1, 3, 5) S\n",
      "Q for a1, s1 0.0 (1, 3, 5) E\n",
      "Q for a1, s1 0.0 (1, 3, 5) W\n",
      "Q for a1, s1 0.0 (1, 3, 5) U\n",
      "Q for a1, s1 0.0 (1, 3, 5) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 4, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 3, 5) N\n",
      "Q[s, a] for s and a: -0.005 (1, 3, 5) N\n",
      "Q for a1, s1 0.0 (1, 4, 5) N\n",
      "Q for a1, s1 0.0 (1, 4, 5) S\n",
      "Q for a1, s1 0.0 (1, 4, 5) E\n",
      "Q for a1, s1 0.0 (1, 4, 5) W\n",
      "Q for a1, s1 0.0 (1, 4, 5) U\n",
      "Q for a1, s1 0.0 (1, 4, 5) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(1, 4, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 4, 5) D\n",
      "Q[s, a] for s and a: -0.005 (1, 4, 5) D\n",
      "Q for a1, s1 0.0 (1, 4, 4) N\n",
      "Q for a1, s1 0.0 (1, 4, 4) S\n",
      "Q for a1, s1 0.0 (1, 4, 4) E\n",
      "Q for a1, s1 0.0 (1, 4, 4) W\n",
      "Q for a1, s1 0.0 (1, 4, 4) U\n",
      "Q for a1, s1 0.0 (1, 4, 4) D\n",
      "policy: E\n",
      "action: E\n",
      "E\n",
      "s1:\n",
      "(2, 4, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 4, 4) E\n",
      "Q[s, a] for s and a: -0.005 (1, 4, 4) E\n",
      "Q for a1, s1 0.0 (2, 4, 4) N\n",
      "Q for a1, s1 0.0 (2, 4, 4) S\n",
      "Q for a1, s1 0.0 (2, 4, 4) E\n",
      "Q for a1, s1 0.0 (2, 4, 4) W\n",
      "Q for a1, s1 0.0 (2, 4, 4) U\n",
      "Q for a1, s1 0.0 (2, 4, 4) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(2, 4, 3)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 4, 4) D\n",
      "Q[s, a] for s and a: -0.005 (2, 4, 4) D\n",
      "Q for a1, s1 0.0 (2, 4, 3) N\n",
      "Q for a1, s1 0.0 (2, 4, 3) S\n",
      "Q for a1, s1 0.0 (2, 4, 3) E\n",
      "Q for a1, s1 0.0 (2, 4, 3) W\n",
      "Q for a1, s1 0.0 (2, 4, 3) U\n",
      "Q for a1, s1 0.0 (2, 4, 3) D\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(2, 4, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 4, 3) U\n",
      "Q[s, a] for s and a: -0.005 (2, 4, 3) U\n",
      "Q for a1, s1 0.0 (2, 4, 4) N\n",
      "Q for a1, s1 0.0 (2, 4, 4) S\n",
      "Q for a1, s1 0.0 (2, 4, 4) E\n",
      "Q for a1, s1 0.0 (2, 4, 4) W\n",
      "Q for a1, s1 0.0 (2, 4, 4) U\n",
      "Q for a1, s1 -0.005 (2, 4, 4) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(2, 5, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 4, 4) N\n",
      "Q[s, a] for s and a: -0.005 (2, 4, 4) N\n",
      "Q for a1, s1 0.0 (2, 5, 4) N\n",
      "Q for a1, s1 0.0 (2, 5, 4) S\n",
      "Q for a1, s1 0.0 (2, 5, 4) E\n",
      "Q for a1, s1 0.0 (2, 5, 4) W\n",
      "Q for a1, s1 0.0 (2, 5, 4) U\n",
      "Q for a1, s1 0.0 (2, 5, 4) D\n",
      "policy: S\n",
      "action: S\n",
      "S\n",
      "s1:\n",
      "(2, 4, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 5, 4) S\n",
      "Q[s, a] for s and a: -0.005 (2, 5, 4) S\n",
      "Q for a1, s1 -0.005 (2, 4, 4) N\n",
      "Q for a1, s1 0.0 (2, 4, 4) S\n",
      "Q for a1, s1 0.0 (2, 4, 4) E\n",
      "Q for a1, s1 0.0 (2, 4, 4) W\n",
      "Q for a1, s1 0.0 (2, 4, 4) U\n",
      "Q for a1, s1 -0.005 (2, 4, 4) D\n",
      "policy: S\n",
      "action: S\n",
      "S\n",
      "s1:\n",
      "(2, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 4, 4) S\n",
      "Q[s, a] for s and a: -0.005 (2, 4, 4) S\n",
      "Q for a1, s1 0.0 (2, 3, 4) N\n",
      "Q for a1, s1 0.0 (2, 3, 4) S\n",
      "Q for a1, s1 0.0 (2, 3, 4) E\n",
      "Q for a1, s1 0.0 (2, 3, 4) W\n",
      "Q for a1, s1 0.0 (2, 3, 4) U\n",
      "Q for a1, s1 0.0 (2, 3, 4) D\n",
      "policy: E\n",
      "action: E\n",
      "E\n",
      "s1:\n",
      "(3, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 3, 4) E\n",
      "Q[s, a] for s and a: -0.005 (2, 3, 4) E\n",
      "Q for a1, s1 0.0 (3, 3, 4) N\n",
      "Q for a1, s1 0.0 (3, 3, 4) S\n",
      "Q for a1, s1 0.0 (3, 3, 4) E\n",
      "Q for a1, s1 0.0 (3, 3, 4) W\n",
      "Q for a1, s1 0.0 (3, 3, 4) U\n",
      "Q for a1, s1 0.0 (3, 3, 4) D\n",
      "mdp.states[148]:\n",
      "(1, 2, 5)\n",
      "trial:  2\n",
      "policy: S\n",
      "action: S\n",
      "S\n",
      "s1:\n",
      "(1, 1, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 5) S\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 5) S\n",
      "Q for a1, s1 0.0 (1, 1, 5) N\n",
      "Q for a1, s1 0.0 (1, 1, 5) S\n",
      "Q for a1, s1 0.0 (1, 1, 5) E\n",
      "Q for a1, s1 0.0 (1, 1, 5) W\n",
      "Q for a1, s1 0.0 (1, 1, 5) U\n",
      "Q for a1, s1 0.0 (1, 1, 5) D\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(1, 1, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 1, 5) U\n",
      "Q[s, a] for s and a: -0.005 (1, 1, 5) U\n",
      "Q for a1, s1 0.0 (1, 1, 6) N\n",
      "Q for a1, s1 0.0 (1, 1, 6) S\n",
      "Q for a1, s1 0.0 (1, 1, 6) E\n",
      "Q for a1, s1 -0.005 (1, 1, 6) W\n",
      "Q for a1, s1 0.0 (1, 1, 6) U\n",
      "Q for a1, s1 0.0 (1, 1, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 2, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 1, 6) N\n",
      "Q[s, a] for s and a: -0.005 (1, 1, 6) N\n",
      "Q for a1, s1 0.0 (1, 2, 6) N\n",
      "Q for a1, s1 -0.005 (1, 2, 6) S\n",
      "Q for a1, s1 0.0 (1, 2, 6) E\n",
      "Q for a1, s1 0.0 (1, 2, 6) W\n",
      "Q for a1, s1 0.0 (1, 2, 6) U\n",
      "Q for a1, s1 0.0 (1, 2, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 3, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 6) N\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 6) N\n",
      "Q for a1, s1 0.0 (1, 3, 6) N\n",
      "Q for a1, s1 0.0 (1, 3, 6) S\n",
      "Q for a1, s1 0.0 (1, 3, 6) E\n",
      "Q for a1, s1 0.0 (1, 3, 6) W\n",
      "Q for a1, s1 0.0 (1, 3, 6) U\n",
      "Q for a1, s1 0.0 (1, 3, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 4, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 3, 6) N\n",
      "Q[s, a] for s and a: -0.005 (1, 3, 6) N\n",
      "Q for a1, s1 0.0 (1, 4, 6) N\n",
      "Q for a1, s1 0.0 (1, 4, 6) S\n",
      "Q for a1, s1 0.0 (1, 4, 6) E\n",
      "Q for a1, s1 0.0 (1, 4, 6) W\n",
      "Q for a1, s1 0.0 (1, 4, 6) U\n",
      "Q for a1, s1 0.0 (1, 4, 6) D\n",
      "policy: E\n",
      "action: E\n",
      "E\n",
      "s1:\n",
      "(2, 4, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 4, 6) E\n",
      "Q[s, a] for s and a: -0.005 (1, 4, 6) E\n",
      "Q for a1, s1 0.0 (2, 4, 6) N\n",
      "Q for a1, s1 0.0 (2, 4, 6) S\n",
      "Q for a1, s1 0.0 (2, 4, 6) E\n",
      "Q for a1, s1 0.0 (2, 4, 6) W\n",
      "Q for a1, s1 0.0 (2, 4, 6) U\n",
      "Q for a1, s1 0.0 (2, 4, 6) D\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(2, 4, 7)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 4, 6) U\n",
      "Q[s, a] for s and a: -0.005 (2, 4, 6) U\n",
      "Q for a1, s1 0.0 (2, 4, 7) N\n",
      "Q for a1, s1 0.0 (2, 4, 7) S\n",
      "Q for a1, s1 0.0 (2, 4, 7) E\n",
      "Q for a1, s1 0.0 (2, 4, 7) W\n",
      "Q for a1, s1 0.0 (2, 4, 7) U\n",
      "Q for a1, s1 0.0 (2, 4, 7) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(2, 4, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 4, 7) D\n",
      "Q[s, a] for s and a: -0.005 (2, 4, 7) D\n",
      "Q for a1, s1 0.0 (2, 4, 6) N\n",
      "Q for a1, s1 0.0 (2, 4, 6) S\n",
      "Q for a1, s1 0.0 (2, 4, 6) E\n",
      "Q for a1, s1 0.0 (2, 4, 6) W\n",
      "Q for a1, s1 -0.005 (2, 4, 6) U\n",
      "Q for a1, s1 0.0 (2, 4, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(2, 5, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 4, 6) N\n",
      "Q[s, a] for s and a: -0.005 (2, 4, 6) N\n",
      "Q for a1, s1 0.0 (2, 5, 6) N\n",
      "Q for a1, s1 0.0 (2, 5, 6) S\n",
      "Q for a1, s1 0.0 (2, 5, 6) E\n",
      "Q for a1, s1 0.0 (2, 5, 6) W\n",
      "Q for a1, s1 0.0 (2, 5, 6) U\n",
      "Q for a1, s1 0.0 (2, 5, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(2, 6, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 5, 6) N\n",
      "Q[s, a] for s and a: -0.005 (2, 5, 6) N\n",
      "Q for a1, s1 0.0 (2, 6, 6) N\n",
      "Q for a1, s1 0.0 (2, 6, 6) S\n",
      "Q for a1, s1 0.0 (2, 6, 6) E\n",
      "Q for a1, s1 0.0 (2, 6, 6) W\n",
      "Q for a1, s1 0.0 (2, 6, 6) U\n",
      "Q for a1, s1 0.0 (2, 6, 6) D\n"
     ]
    }
   ],
   "source": [
    "# Solution Part D:\n",
    "\n",
    "size = 11\n",
    "default_reward = -0.01\n",
    "discount = 0.999\n",
    "actions = ['N','S','E','W','U','D']\n",
    "mdp = MDPLanding(size, default_reward, actions, discount)\n",
    "n_trials = 3 #homework 3, part 2\n",
    "Q,Nsa,rewards = run_many_trials(mdp, n_trials, n_steps=10)\n",
    "#Q,Nsa,rewards = run_many_trials(mdp, n_trials=1, n_steps=50)\n",
    "\n",
    "\n",
    "#di = 500\n",
    "#rewards_block = list(np.mean(np.asarray(rewards).reshape(-1, di),1))\n",
    "#plt.plot(list(range(0,len(rewards),di)), rewards_block)\n",
    "#plt.xlabel('Training episode')\n",
    "#plt.ylabel('Cumulative reward')\n",
    "#plt.show()\n",
    "\n",
    "#print('Last 5000 trials mean cumulative reward = {:0.4f}'.format(np.mean(rewards[-5000:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E - Homework 3, part 2 questions.\n",
    "These questions refer to the Q-learning implementation in Parts C and D of this notebook. \n",
    "\n",
    "**1:** (1 pts) This code in Part D is set up to run 3 trials, 10 steps each, starting from (1,2,5). The values for state, reward, policy, and Q[s,a] print on each step. \n",
    "- What is the first action taken and what state does the agent move to? \n",
    "- The values for policy and action are printed. What's the difference in the code?\n",
    "- What are the lines of code that implement the action and determine the new state?\n",
    "\n",
    "**2:** (1 pts) How is the s1 variable used in the code, including in the Q[s,a] calculation?\n",
    "\n",
    "**3:** (1 pt) What are the initial values for the Q-matrix, the discount factor, learning rate, and the policy matrix?\n",
    "\n",
    "**4:** (2 pts) What is the purpose of the *max(Q[s1, a1] for a1 in mdp.actions(s1))* code in the Q[s,a] calculation? How does it influence the Q[s,a] value?\n",
    "\n",
    "**5:** (2 pts) Where does the code store the policy for each state? How is it updated and where is it used in the code?\n",
    "\n",
    "**6:** (3 pts) Provide a 1-2 paragraph description of the differences in your Q-learning algorithm with the solution provided. Be specific. If there were parts of your code that didn't work, describe how your code was different than the solution. Even if your code did work, it can still be beneficial to see how someone else implemented a solution to the same problem and analyze the similarities and differences.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.**\n",
    "\n",
    "* the first action taken is U, which moves the agent 1 unit in the positive z direction. The new state is (1, 2, 6)\n",
    "* The value for policy is a random action that the agent can take in that particular state, while value for action is determined by epsilon value. If the probability is less than or equals to the epsilon value, a random action that the agent can take in that state is returned, else the action which returns the highest utility is used.\n",
    "* s1 = mdp.result(s, a): this line implements the action and determines the new state.\n",
    "\n",
    "**2.** s1 is used to find the new reward, r1, associated with the state. <br>\n",
    "s1 is also used in the Q[s, a] equation to find the new Q value, by determining the maximum adjustment Q value when using new s and a values.\n",
    "\n",
    "**3.** Initial values for the Q matrix: Q[s, a] = 0, s = (1, 2, 5), a: U <br>\n",
    "discount factor = 0.999 <br>\n",
    "learning rate = 1/(1+1) = 0.5 <br>\n",
    "policy: a dictionary, where the key is each state and value is a random action out of all possible actions in the mdp.\n",
    "\n",
    "**4.** It is to find the maximum Q value after looping through all possible actions for the new state. It is an adjustment value by using knowledge of surrounding state utilities to generate the correct utility estimates. The new Q value will be closer to the actual value.\n",
    "\n",
    "**5.** The code for the policy is stored in the policy variable, which is a dictionary, with the key being all the states in the mdp, and the value is an action for the state. It is used in the Greedy function, by returning the action with the highest utility. It is updated by looping through all the actions that can be taken from the current state and finding the action that gives the highest Q value.\n",
    "\n",
    "**6.** In my greedy function, I returned the action that generates the highest q value. However, in the solution provided, it returns the action with the highest utility by calling the dictionary directly. <br>\n",
    "In my QLearning function, I manually inputted the learning rate. However, in the solution provided, the learning rate = 1/(1+n), where n is the number of times this state is visited. Hence, the learning rate changes for each state, but in my code, the learning rate is fixed. The solution also included a policy variable and a state-action visit counter which I did not include in my code. There was also code to update the policy variable in the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
